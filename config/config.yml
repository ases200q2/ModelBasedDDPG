general:
  name:
  random_seed: 123
#  random_seed: 456
#  random_seed: 789
  updates_cycle_count: 10000
  episodes_per_update: 16
  model_updates_per_cycle: 40
#  model_updates_per_cycle: 400
  max_path_slack: 1.5
  gpu_usage: 0.01
  actor_gpu_usage: 0.01
#  actor_processes: 2
#  collector_processes: 4
  actor_processes: 6
  collector_processes: 12
#  actor_processes: 1
#  collector_processes: 1
  write_train_summaries: 500
  save_model_every_cycles: 100
#  params_file: # for no params
#  params_file: 'params_simple.pkl' # simple workspace
  params_file: 'params_hard.pkl' # hard workspace

openrave_rl:
  action_step_size: 0.025
  segment_validity_step: 0.001
  goal_sensitivity: 0.04
  planner_iterations_start: 100
  planner_iterations_increase: 10
  planner_iterations_decrease: 1
  keep_alive_penalty: 0.01
  truncate_penalty: 0.05
  challenging_trajectories_only: True
#  challenging_trajectories_only: False

model:
  buffer_size: 1000000
#  batch_size: 128
  batch_size: 512
  gamma: 0.99
  potential_points: [5, -0.02, 0.035]
#  potential_points: [2, 0., 0.075, 3, 0., 0.085, 4, -0.02, 0.05, 4, 0.005, 0.05, 5, 0.005, 0.035, 5, -0.02, 0.035]
  tau: 0.05
#  tau: 0.01
  consider_image: False
  consider_goal_pose: True
  random_action_probability: 0.2
  random_noise_std: 0.05
  use_reward_model: True
#  use_reward_model: False
  reward_model_name: '2018_11_25_10_22_02' # hard workspace with transition label
#  reward_model_name: '2018_11_10_09_02_38' # hard workspace
#  reward_model_name: '2018_10_31_10_44_31' # easy workspace
#  alter_episode: 0  # natural reward and original episode
#  alter_episode: 1  # use learned reward with episode truncation
  alter_episode: 2  # use learned reward without episode truncation
#  failed_motion_planner_trajectories: 16
  failed_motion_planner_trajectories: 2
#  failed_motion_planner_trajectories: 0

test:
  test_every_cycles: 50
  number_of_episodes: 50

hindsight:
#  enable: False
  enable: True
#  type: 'goal'
  type: 'future'
  k: 4
  score_with_reward_model: True
#  score_with_reward_model: False

curriculum:
  enable: False
#  enable: True
  initial_length: 0.1
  length_increments: 0.02
  success_rate_increase: 0.8
  minimal_episodes: 10

actor:
  learning_rate: 0.001
#  gradient_limit: 0.0
  gradient_limit: 0.01

action_predictor:
  layers: [200, 200, 200, 200]
#  layers: [64, 64, 64]
  activation: 'elu'
  tanh_preactivation_loss_coefficient: 1.0
#  tanh_preactivation_loss_coefficient: 0.0

critic:
  learning_rate: 0.001
#  learning_rate: 0.01
  gradient_limit: 1.0
#  gradient_limit: 0.0
  layers_before_action: [200, 200]
  layers_after_action: [200, 200, 200]
#  layers_before_action: [400, 400]
#  layers_after_action: [400, 400, 400]
#  layers_before_action: [64]
#  layers_after_action: [64, 64]
  activation: 'elu'
#  l2_regularization_coefficient: 0.0
#  l2_regularization_coefficient: 0.000001
  l2_regularization_coefficient: 0.0001
#  last_layer_tanh: True
  last_layer_tanh: False

reward:
  learning_rate: 0.001
  gradient_limit: 0.0
  activation: 'elu'
  layers: [50, 50, 50]
  l2_regularization_coefficient: 0.0




