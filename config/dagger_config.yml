general:
  epochs: 100
  save_every_epochs: 5
  gpu_usage: 0.01
  actor_gpu_usage: 0.01
  actor_processes: 8
#  actor_processes:
#  actor_processes: 1
  openrave_processes: 10
  max_path_slack: 1.5
  scenario: 'simple'
#  scenario: 'hard'
  train_files: 160
  train_transition_files: 10
  test_files: 40

test:
  number_of_episodes: 200

openrave_rl:
  action_step_size: 0.025
  segment_validity_step: 0.001
  goal_sensitivity: 0.04
  planner_iterations: 100
  planner_iterations_start: 100
  planner_iterations_increase: 10
  planner_iterations_decrease: 1
  keep_alive_penalty: 0.01
  truncate_penalty: 0.05

model:
  batch_size: 1024
  potential_points: [5, -0.02, 0.035]
  consider_image: False
  dagger_episodes_per_epoch: 50

action_predictor:
  layers: [200, 200, 200, 200]
#  layers: [64, 64, 64]
  activation: 'elu'
  tanh_preactivation_loss_coefficient: 1.0
#  tanh_preactivation_loss_coefficient: 0.0

imitation:
#  initial_learn_rate: 0.001
  initial_learn_rate: 0.0001
  decrease_learn_rate_after: 100000
  learn_rate_decrease_rate: 1.0
#  learning_rate: 0.0001
  gradient_limit: 5.0
#  gradient_limit: 0.0


